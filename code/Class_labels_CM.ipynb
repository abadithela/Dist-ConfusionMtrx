{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7e38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.783 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.778 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.724 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Class based distance parametrized confusion matrices\n",
    "# Apurva Badithela\n",
    "# 6/7/22\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "from utils import *\n",
    "dataroot=\"/Users/apurvabadithela/Documents/software/nuscenes/data/sets/nuscenes\"\n",
    "from nuscenes.nuscenes import NuScenes, NuScenesExplorer\n",
    "from yolo_bboxes import *\n",
    "from shapely.geometry import MultiPoint, box\n",
    "import pickle as pkl\n",
    "from parse_matchings import ConfusionMatrix, cluster_categories\n",
    "import numpy as np\n",
    "\n",
    "nusc = NuScenes(dataroot=\"/Users/apurvabadithela/Documents/software/nuscenes/data/sets/nuscenes/\")\n",
    "namesfile = 'data/coco.names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5296a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the COCO object classes\n",
    "class_names = load_class_names(namesfile)\n",
    "nms_thresh = 0.6\n",
    "iou_thresh = 0.4\n",
    "horizon = 120\n",
    "discrete_horizon = 5\n",
    "sensor = 'CAM_FRONT' # Sensor rleevant in this study\n",
    "PLOT = True\n",
    "Nclasses = 3\n",
    "class_dict = {0: 'pedestrian', 1:'obstacle', 2:'empty'}\n",
    "\n",
    "\n",
    "# Cluster categories:\n",
    "categories = []\n",
    "for category_dict in nusc.category:\n",
    "    cat_name = category_dict['name']\n",
    "    if cat_name not in categories:\n",
    "        categories.append(cat_name)\n",
    "sup_categories = cluster_categories(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f35103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all instances captured in the first annotation of the object:\n",
    "def get_yolo_object_type(box, class_names):\n",
    "    # print('Objects Found and Confidence Level:\\n')\n",
    "    if len(box) >= 7 and class_names:\n",
    "        cls_conf = box[5]\n",
    "        cls_id = box[6]\n",
    "        return class_names[cls_id], cls_conf\n",
    "\n",
    "# Function to get yOlO Boxes:\n",
    "def get_yolo_boxes(yolo, data_path):\n",
    "    boxes, boxes_pixels_yolo = yolo.yolo(data_path)\n",
    "    return boxes, boxes_pixels_yolo\n",
    "\n",
    "# function to get nuscenes boxes:\n",
    "def get_gt_boxes(sample):\n",
    "    # Get front camera data:\n",
    "    cam_front_data_f = nusc.get('sample_data', sample['data'][sensor])\n",
    "    data_path_f, boxes, camera_intrinsic = nusc.get_sample_data(cam_front_data_f['token'], box_vis_level=BoxVisibility.ANY)\n",
    "    print(\"Accessed data for ground truth boxes\")\n",
    "    boxes_gt_pixels = box_nusc(boxes, camera_intrinsic)\n",
    "    boxes_gt = []\n",
    "    return boxes_gt_pixels, boxes, data_path_f, cam_front_data_f, camera_intrinsic\n",
    "\n",
    "def prepare_gt_box(nubox):\n",
    "    xmin = nubox[0]\n",
    "    ymin = nubox[1]\n",
    "    xmax = nubox[2]\n",
    "    ymax = nubox[3]\n",
    "    new_nusc_box = [xmin, ymin, xmax-xmin, ymin-ymax]\n",
    "    return new_nusc_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfac243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare boxes:\n",
    "# To-DO: Cleanup code\n",
    "def compare_boxes(boxes_gt, boxes_yolo_pixels):\n",
    "    matchings = {}\n",
    "    matched_nusc_boxes = []\n",
    "    matched_yolo_boxes = []\n",
    "    boxes_gt_dict = {}\n",
    "    boxes_yolo_dict = {}\n",
    "    i = 0\n",
    "    for box_gt in boxes_gt:\n",
    "        boxes_gt_dict[i] = prepare_gt_box(box_gt)\n",
    "        matchings[i] = dict()\n",
    "        matchings[i][\"yolo_match\"] = dict()\n",
    "        i += 1\n",
    "    i= 0\n",
    "    for box_yolo in boxes_yolo_pixels:\n",
    "        boxes_yolo_dict[i] = prepare_yolo_box(box_yolo)\n",
    "        i += 1\n",
    "\n",
    "    for box_gt_i in boxes_gt_dict.keys():\n",
    "        for box_yolo_i in boxes_yolo_dict.keys():\n",
    "            box_gt = boxes_gt_dict[box_gt_i]\n",
    "            box_yolo = boxes_yolo_dict[box_yolo_i]\n",
    "            iou = compute_iou(box_gt, box_yolo)\n",
    "\n",
    "            if iou >= 0.7:\n",
    "                matchings[box_gt_i][\"yolo_match\"][\"box_id\"] = box_yolo_i\n",
    "                matched_nusc_boxes.append(boxes_gt_dict[box_gt_i])\n",
    "                matched_yolo_boxes.append(boxes_yolo_dict[box_yolo_i])\n",
    "                print(\"Match found\")\n",
    "    return matchings, matched_nusc_boxes, matched_yolo_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d3c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process list of observations seen at a distance j:\n",
    "# ann_at_j is a list of predictions for the ground truth annotation, gt_ann, at the same distance in a single image\n",
    "\n",
    "def process_anns_at_j(prediction, gt_ann, scene_cmi):\n",
    "    assert prediction!=[]\n",
    "    # class_dict = {0: 'pedestrian', 1:'vehicle', 2:'obstacle', 3:'empty'}\n",
    "    rev_class_dict = {v: k for k, v in class_dict.items()}\n",
    "    correctly_predicted = [gt_ann == p for p in prediction]\n",
    "    unique_predictions = list(set(prediction))\n",
    "    if any(correctly_predicted):\n",
    "        row = rev_class_dict[gt_ann]\n",
    "        col = rev_class_dict[gt_ann]\n",
    "        scene_cmi[row, col] += 1\n",
    "    else:\n",
    "        for pred in unique_predictions:\n",
    "            row = rev_class_dict[pred]\n",
    "            col = rev_class_dict[gt_ann]\n",
    "            scene_cmi[row, col] += 1.0\n",
    "    return scene_cmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3a6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct the confusion matrix from the categorized annotations :\n",
    "def construct_confusion_matrix(scene_obj):\n",
    "    discretization = np.linspace(discrete_horizon, horizon, int(horizon/discrete_horizon))\n",
    "    scene_cm = {i: np.zeros((Nclasses,Nclasses)) for i in range(len(discretization))} # distionary of confusion\n",
    "    \n",
    "    # class_dict = {0: 'pedestrian', 1:'vehicle', 2:'obstacle', 3:'empty'}\n",
    "    class_dict = {0: 'pedestrian', 1:'obstacle', 2:'empty'}\n",
    "    rev_class_dict = {v: k for k, v in class_dict.items()}\n",
    "    obj_det=False\n",
    "    \n",
    "    # Discretization of the state space\n",
    "    for i in range(len(discretization)): \n",
    "        objs_at_dist_i = scene_obj[i]\n",
    "        if objs_at_dist_i == []: # If no object at that distance return empty\n",
    "            true_class = 'empty'\n",
    "            pred_class = 'empty'\n",
    "            row = rev_class_dict[pred_class]\n",
    "            col = rev_class_dict[true_class]\n",
    "            scene_cm[i][row, col] += 1\n",
    "        else:\n",
    "            obj_det = True\n",
    "            pred_ped = [obj[1] for obj in objs_at_dist_i if obj[0]=='pedestrian']\n",
    "            # pred_veh = [obj[1] for obj in objs_at_dist_i if obj[0]=='vehicle']\n",
    "            pred_obs = [obj[1] for obj in objs_at_dist_i if obj[0]=='obstacle']\n",
    "            if pred_ped:\n",
    "                scene_cm[i] = process_anns_at_j(pred_ped, 'pedestrian', scene_cm[i])\n",
    "            # if pred_veh:\n",
    "            #     scene_cm[i] = process_anns_at_j(pred_veh, 'vehicle', scene_cm[i])\n",
    "            if pred_obs:\n",
    "                scene_cm[i] = process_anns_at_j(pred_obs, 'obstacle', scene_cm[i])\n",
    "    return scene_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bbb25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene confusion matrix:\n",
    "# scene_cm is an N-by-N matrix where scene_cm(i,j) has the ith location of the\n",
    "def scene_confusion_matrix(distance_to_annotations, matchings, boxes_nusc):\n",
    "    discretization = np.linspace(discrete_horizon, horizon, int(horizon/discrete_horizon))\n",
    "    scene_obj = {i: [] for i in range(len(discretization))} # distionary of confusion matrix by distance\n",
    "    # Categorize ground truth annotations based on their distance from ego:\n",
    "    for j in range(len(boxes_nusc)):\n",
    "        nubox = boxes_nusc[j] # Box object\n",
    "        distance_to_ego = distance_to_annotations[nubox.token]\n",
    "        if any(distance_to_ego < discretization):\n",
    "            dindx = np.where(distance_to_ego < discretization)[0][0] # First index\n",
    "            true_class = sup_categories[matchings[j]['category']]\n",
    "            if matchings[j][\"yolo_match\"]:\n",
    "                pred_class = matchings[j]['yolo_match']['pred_class'] # Find the classified matching, otherwise empty\n",
    "            else:\n",
    "                pred_class = 'empty'\n",
    "            scene_obj[dindx].append((true_class, pred_class))\n",
    "    scene_cm = construct_confusion_matrix(scene_obj)\n",
    "    return scene_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "677c5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance to ego\n",
    "def compute_distance_to_ego(boxes_nusc, cam_data):\n",
    "    sample_token = cam_data['token']\n",
    "    sd_record = nusc.get('sample_data', sample_token)\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    curr_sample_record = nusc.get('sample', sd_record['sample_token'])\n",
    "    curr_ann_recs = [nusc.get('sample_annotation', token) for token in curr_sample_record['anns']]\n",
    "    cam_front_boxes_nusc = list([box.token for box in boxes_nusc])\n",
    "    cam_front_anns = [ann_rec for ann_rec in curr_ann_recs if curr_ann_recs in cam_front_boxes_nusc] # only look at front camera annotations:\n",
    "    # Sanity check:\n",
    "    assert cam_front_anns!=None\n",
    "\n",
    "    # Move box to ego vehicle coordinate system from sensor coordinate system:\n",
    "    distance_to_ego = dict()\n",
    "    boxes_copy = boxes_nusc.copy()\n",
    "    for box in boxes_copy:\n",
    "        box.rotate(Quaternion(cs_record['rotation']))\n",
    "        box.translate(np.array(cs_record['translation']))\n",
    "        box_pos = box.center\n",
    "        distance_to_ego[box.token] = np.linalg.norm(box_pos)\n",
    "    return distance_to_ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f390fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10ading weights. Please Wait...100.00% Complete\n",
      "Iteration ........  1\n",
      "Getting Nuscenes ground truth 2D boxes: \n",
      "Accessed data for ground truth boxes\n",
      "Getting YoLo predicted 2D boxes: \n",
      "\n",
      "\n",
      "It took 1.193 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 4 \n",
      "\n",
      "1600\n",
      "900\n",
      "Modified pixels: \n",
      "416\n",
      "416\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. car: 0.635040\n",
      "2. truck: 0.999807\n",
      "3. car: 0.990484\n",
      "4. car: 0.772251\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Getting Nuscenes ground truth 2D boxes: \n",
      "Accessed data for ground truth boxes\n",
      "Getting YoLo predicted 2D boxes: \n",
      "\n",
      "\n",
      "It took 1.172 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 5 \n",
      "\n",
      "1600\n",
      "900\n",
      "Modified pixels: \n",
      "416\n",
      "416\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. car: 0.724648\n",
      "2. truck: 0.999977\n",
      "3. car: 0.985283\n",
      "4. person: 1.000000\n",
      "5. car: 0.996995\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Getting Nuscenes ground truth 2D boxes: \n",
      "Accessed data for ground truth boxes\n",
      "Getting YoLo predicted 2D boxes: \n",
      "\n",
      "\n",
      "It took 1.110 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 7 \n",
      "\n",
      "1600\n",
      "900\n",
      "Modified pixels: \n",
      "416\n",
      "416\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. truck: 0.677914\n",
      "2. car: 0.603466\n",
      "3. car: 0.744507\n",
      "4. person: 1.000000\n",
      "5. car: 0.966135\n",
      "6. car: 0.999831\n",
      "7. person: 0.999964\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Getting Nuscenes ground truth 2D boxes: \n",
      "Accessed data for ground truth boxes\n",
      "Getting YoLo predicted 2D boxes: \n",
      "\n",
      "\n",
      "It took 1.066 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 6 \n",
      "\n",
      "1600\n",
      "900\n",
      "Modified pixels: \n",
      "416\n",
      "416\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. car: 0.815438\n",
      "2. car: 0.709844\n",
      "3. truck: 0.994939\n",
      "4. car: 0.995031\n",
      "5. car: 0.528746\n",
      "6. car: 0.999959\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Getting Nuscenes ground truth 2D boxes: \n",
      "Accessed data for ground truth boxes\n",
      "Getting YoLo predicted 2D boxes: \n",
      "\n",
      "\n",
      "It took 1.094 seconds to detect the objects in the image.\n",
      "\n",
      "Number of Objects Detected: 5 \n",
      "\n",
      "1600\n",
      "900\n",
      "Modified pixels: \n",
      "416\n",
      "416\n",
      "Objects Found and Confidence Level:\n",
      "\n",
      "1. car: 0.997836\n",
      "2. car: 0.976603\n",
      "3. car: 0.817918\n",
      "4. car: 0.997476\n",
      "5. traffic light: 1.000000\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "Match found\n",
      "> \u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/710634494.py\u001b[0m(21)\u001b[0;36mprocess_anns_at_j\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m            \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrev_class_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgt_ann\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m            \u001b[0mscene_cmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mscene_cmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/1014623304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Construct confusion matrix for this scene:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mscene_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_to_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatchings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_nusc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_conf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_cm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Store all matchings at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/1564013428.py\u001b[0m in \u001b[0;36mscene_confusion_matrix\u001b[0;34m(distance_to_annotations, matchings, boxes_nusc)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'empty'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mscene_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mscene_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscene_cm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/239897171.py\u001b[0m in \u001b[0;36mconstruct_confusion_matrix\u001b[0;34m(scene_obj)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mpred_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjs_at_dist_i\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'obstacle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpred_ped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mscene_cm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_anns_at_j\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pedestrian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_cm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# if pred_veh:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m#     scene_cm[i] = process_anns_at_j(pred_veh, 'vehicle', scene_cm[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/710634494.py\u001b[0m in \u001b[0;36mprocess_anns_at_j\u001b[0;34m(prediction, gt_ann, scene_cmi)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mscene_cmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscene_cmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/k2w5mcvs7_xfg7cxqbdjfdzr0000gn/T/ipykernel_7274/710634494.py\u001b[0m in \u001b[0;36mprocess_anns_at_j\u001b[0;34m(prediction, gt_ann, scene_cmi)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mscene_cmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscene_cmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nuscene/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nuscene/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterating over scenes\n",
    "# print(nusc.list_sample)\n",
    "if __name__ == '__main__':\n",
    "    classes = [\"pedestrian\",  \"obstacle\"]\n",
    "    horizon = 100\n",
    "    discrete_horizon = 10\n",
    "    distance_bins = int(horizon/discrete_horizon)\n",
    "    C = ConfusionMatrix(classes, horizon, distance_bins=distance_bins)\n",
    "    Nscenes = len(nusc.scene)\n",
    "\n",
    "    # instantiating YoLo obkect:\n",
    "    yolo = YoLo( nms_thresh, iou_thresh)\n",
    "    print(Nscenes)\n",
    "    for n in range(1,Nscenes+1):\n",
    "        print(\"Iteration ........ \", str(n))\n",
    "        objects_detected = dict() # Matchings over objects detected\n",
    "        scene = nusc.scene[n-1]\n",
    "        # iterating over timestamps / samples:\n",
    "        sample_token = scene['first_sample_token']\n",
    "        sample = nusc.get('sample', scene['first_sample_token'])\n",
    "        sample_number = 1\n",
    "\n",
    "        while sample_number < scene['nbr_samples']:\n",
    "            # Get ground truth 2D boxes for front camera:\n",
    "            print(\"Getting Nuscenes ground truth 2D boxes: \")\n",
    "            boxes_gt_pixels, boxes_nusc, data_path, cam_data, camera_intrinsic = get_gt_boxes(sample)\n",
    "            # Get Yolo Boxes:\n",
    "            print(\"Getting YoLo predicted 2D boxes: \")\n",
    "            boxes_yolo, boxes_yolo_pixels = get_yolo_boxes(yolo, data_path)\n",
    "\n",
    "            # Compare bounding boxes:\n",
    "            matchings, matched_gt_boxes, matched_yolo_boxes = compare_boxes(boxes_gt_pixels, boxes_yolo_pixels)\n",
    "            # compute distance from ego to annotations:\n",
    "\n",
    "            distance_to_annotations = compute_distance_to_ego(boxes_nusc, cam_data)\n",
    "\n",
    "            # Add annotations:\n",
    "            for j in range(len(boxes_nusc)):\n",
    "                nubox = boxes_nusc[j] # Box object\n",
    "                matchings[j]['nusc_token'] = nubox.token\n",
    "                matchings[j]['distance_to_ego'] = distance_to_annotations[nubox.token] # Distance to ego\n",
    "                matchings[j]['category'] = nubox.name\n",
    "\n",
    "                if matchings[j][\"yolo_match\"]:\n",
    "                    yolo_id = matchings[j][\"yolo_match\"][\"box_id\"]\n",
    "                    yolo_box_id = boxes_yolo[yolo_id]\n",
    "                    if class_names[yolo_box_id[6]] in [\"car\", \"truck\", \"bus\"]:\n",
    "                        matchings[j][\"yolo_match\"][\"pred_class\"] = \"obstacle\"\n",
    "                    elif class_names[yolo_box_id[6]] == \"person\":\n",
    "                        matchings[j][\"yolo_match\"][\"pred_class\"] = \"pedestrian\"\n",
    "                    else:\n",
    "                        matchings[j][\"yolo_match\"][\"pred_class\"] = \"obstacle\"\n",
    "\n",
    "            # Construct confusion matrix for this scene:\n",
    "            scene_cm = scene_confusion_matrix(distance_to_annotations, matchings, boxes_nusc)\n",
    "            C.add_conf_matrix(scene_cm)\n",
    "            # Store all matchings at the end\n",
    "            objects_detected[sample_number] = matchings\n",
    "            # Update sample number:\n",
    "            sample_token = sample['next']\n",
    "            sample = nusc.get('sample', sample_token)\n",
    "            sample_number += 1\n",
    "\n",
    "        # Save data:\n",
    "        cwd = os.getcwd()\n",
    "        dirname = cwd + \"/matchings_new\"\n",
    "        # pdb.set_trace()\n",
    "        if os.path.exists(dirname) is False:\n",
    "            os.mkdir(dirname)\n",
    "        fname = dirname + \"/scene_\"+str(n)+\"_matchings.p\"\n",
    "        pkl.dump(objects_detected, open(fname, \"wb\"))\n",
    "\n",
    "    # Print confusion matrix:\n",
    "    C.print_cm()\n",
    "    dirname = cwd + \"/\"\n",
    "    fname = dirname + \"class_labeled_conf_matrix_hrzn_\" + str(horizon) + \"_distbin_\" + str(distance_bins) + \".p\"\n",
    "    pkl.dump(C.C, open(fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cb460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
